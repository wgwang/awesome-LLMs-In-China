# 开源开放的大模型

旨在记录全球开源开放大模型发展情况，欢迎提供
- *线索*
- *材料*
- *PR*
- *Issue*


## 基础大模型
|序号|名称|参数规模|数据规模|说明|
|:-|:-|:-|:-|:-|
|1|[LLaMA-2](Open-LLMs/llama2.md)|7B,13B,34B,70B|2T|可商用|
|2|[Falcon](Open-LLMs/falcon.md)|7B,40B,180B|3.5T|数据集[ RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)|
|3|[baichuan-2](Open-LLMs/baichuan2.md)|7B,13B|2.6T|开放，商用需授权，[baichuan-1](Open-LLMs/baichuan.md)|
|4|[InternLM](Open-LLMs/internlm.md)|7B,20B|2.3T|开放，商用需授权|
|5|[BLOOM](Open-LLMs/bloom.md)|3B,7.1B,176B|366B|可商用，最为宽松，[详细介绍](https://mp.weixin.qq.com/s/ia-yrmXbnlooRA3K1hoTwQ)|
|6|GALACTICA|6.7B,30B,120B|106B|开放的科学文本和数据|
|7|[LLaMA](Open-LLMs/llama.md)|7B,13B,30B,65B|1.4T|Meta，代码开源，模型“泄露”,不可商用，[详细介绍](https://mp.weixin.qq.com/s/dKInMi6P80GXecUtR3WQsA)|
|8|MOSS-moon|16B|700B|6.67x1022 FLOPs|
|9|ChatGLM2|6B|1.4T||
|10|StableLM|3B,7B|800B||
|11|RedPajama-INCITE|3B,7B|1T||
|12|GPT-NeoX|20B|3.15M|800GB的[The Pile](https://arxiv.org/abs/2101.00027)数据集|
|13|OpenLLaMA|3B,7B,13B|1T||
|14|MPT|7B,30B|1T|
|15|Pythia|2.8B,6.9B,12B|300B||
|16|XGen|7B|1.5T||
|17|OPT|6.7B,13B,30B,66B,175B|180B||
|18|[Qwen](Open-LLMs/qwen.md)|7B,14B|2.4T,3.0T||
|19|XVERSE|13B|1.4T||
|20|Aquila|7B||悟道·天鹰|
|21|Prithvi|||IBM+NASA,地理空间，100M（图片）|

## 非基础大模型
- WizardLM，WizardMath，WizardCoder
- Alpaca
- Vicuna
- Guanaco
- [CodeLLaMA](Open-LLMs/codellama.md)
  - 7B,13B,34B



## 模型架构

- [GPTQ](https://github.com/IST-DASLab/gptq)
- [LLaMA](https://github.com/facebookresearch/llama)


